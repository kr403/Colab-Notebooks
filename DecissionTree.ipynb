{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DecissionTree.ipynb","provenance":[],"authorship_tag":"ABX9TyNl1ntzbbzmCyvl7NgVsSFZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"cKCPDs9yEC8b"},"source":["import numpy as np\n","import math\n","import random\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EQvhC7ieErYl"},"source":["#Upload dataset\n","from google.colab import files\n","uploaded = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tkbTzZaNEtyD"},"source":["#Read dataset\n","import io\n","ttt = pd.read_csv(io.BytesIO(uploaded['tic-tac-toe.csv']))\n","ttt = ttt.replace({\n","    #Covert output values to numerical values from categorical values\n","    \"Class\":{\n","        \"negative\":0,\n","        \"positive\":1\n","    }\n","})\n","#Spliting the features and the label\n","ttt_x = ttt.loc[:, ttt.columns != \"Class\"]\n","ttt_y = ttt.loc[:, \"Class\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6tP0GON9mg3_"},"source":["#Split dataset into 80% traning data and 20% testing data\n","train_data, test_data = train_test_split(ttt, train_size=0.8, stratify = ttt_y, random_state=1997)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XBdxiUK-d6Uo"},"source":["class Node:\n","    def __init__(self, attribute=None, attribute_values=None, child_nodes=None, decision=None):\n","        self.attribute = attribute\n","        self.attribute_values = attribute_values\n","        self.child_nodes = child_nodes\n","        self.decision = decision"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DK1MmBQrHOsR"},"source":["class DecisionTree:\n","\n","    root = None\n","\n","    @staticmethod\n","    def plurality_values(data):\n","        labels = data[:, data.shape[1] - 1]  # store the last column in labels\n","        labels = list(labels)\n","        zero = labels.count(0)  # Count the occurence of '0'\n","        one = labels.count(1)  # Count the occurence of '1'\n","        if zero > one:  # Find who occurs more \"0/1\", return 0 if 0 occurs more than 1 and vice versa\n","            return 0\n","        return 1\n","\n","\n","    @staticmethod\n","    def all_zero(data):\n","        labels = data[:, data.shape[1] - 1]  # store the last column in labels\n","        labels = list(labels)\n","        if labels.count(0) == len(labels):  # Check if all the input labels are 0 or not\n","            return True\n","        return False\n","\n","    @staticmethod\n","    def all_one(data):\n","        labels = data[:, data.shape[1] - 1]  # store the last column in labels\n","        labels = list(labels)\n","        if labels.count(1) == len(labels):  # Check if all the input labels are 0 or not\n","            return True\n","        return False\n","\n","    @staticmethod\n","    def importance(data, attributes):\n","        labels = data[:, data.shape[1] - 1]  # store the last column in labels\n","        labels = list(labels)\n","        # Here n is equal to q of the entropy formula [[[B(q) = −q × log2q − (1 − q) × log2(1 − q)]]]\n","        n = labels.count(0)/len(labels)  # Calculating the occurence of 0(n) inside total total labels\n","        # Here p is equal to (1 - q) of the entropy formula [[[B(q) = −q × log2q − (1 − q) × log2(1 − q)]]]\n","        p = labels.count(1)/len(labels)  # Calculating the occurence of 1(p) inside total total labels\n","        l_e = -n*math.log(n, 2) - p*math.log(p, 2)  # Calculating the label entropy where logarithmic base is 2\n","        gain = {}  # Create a blank gain dictonary to store the gain value\n","        length = len(attributes)  # Calculate the length of each attribute\n","        for a in attributes:  # Iterate the attributes elements one by one\n","            x = data[:, data.shape[1] - length - 1]  #Separate each column based on the current length value\n","            length -= 1  #Decrement the length value by 1 over every iteration\n","            uv = np.unique(x)  # Findout all unique values in each column\n","            x = list(x)  # Convert each column into a list\n","            pair = list(zip(x, labels))  # Zip each column with list and convert it into a list\n","            after_split = 0  # Initializing after_split with value 0\n","            for i in uv:  # Iterate through uv by each unique value\n","                y = (i, 1)  # Make a touple with the value of unique value and 1\n","                n = (i, 0)  # Make a touple with the value of unique value and 0\n","                yes = pair.count(y)  # Count the occurence of unique value and 1 in the pair list\n","                no = pair.count(n)  # Count the occurence of unique value and 0 in the pair list\n","                if yes == 0 or no == 0:  # If yes or no is equal to 0 thats mean the entropy is 0\n","                    entropy = 0\n","                else:\n","                    p_yes = yes/(yes + no)  # Calculate the value of q = (p_yes) for the entropy formula\n","                    p_no = no/(yes + no)  # Calculate the value of (1 - q) = (p_yes) for the entropy formula\n","                    entropy = -p_yes*math.log(p_yes, 2) - p_no*math.log(p_no, 2)  # Calculate the entropy\n","                weight = (yes + no)/len(labels)  # Calculate the weight value based on the summation of yes and no\n","                after_split += weight*entropy  # Update the after_split value for next iteration\n","            gain_value = l_e - after_split  # Calculate the gain_value\n","            if a not in gain:  # If the attribute element absent in gain dictonary then add the value in that dictonary and the key is the element of the attribute\n","                gain[a] = gain_value\n","        max = -1000  # Assume that the max value of gain is negative 1000\n","        atr = -1  # Assume that the atr value of gain is negative 1\n","        for value in gain:  # Iterate gain dictonary by value\n","            if gain[value] > max:  # Compare if the real gain value is greater than max\n","                max = gain[value]  # Upadate the max value with real gain value\n","                atr = value  # Upadate the atr value with real value from gain dictonary\n","        return atr  # Return the final atr value\n","\n","\n","    def train(self, data, attributes, parent_data):\n","        data = np.array(data)\n","        parent_data = np.array(parent_data)\n","        attributes = list(attributes)\n","\n","        if data.shape[0] == 0:  # if x is empty\n","            return Node(decision=self.plurality_values(parent_data))\n","\n","        elif self.all_zero(data):\n","            return Node(decision=0)\n","\n","        elif self.all_one(data):\n","            return Node(decision=1)\n","\n","        elif len(attributes) == 0:\n","            return Node(decision=self.plurality_values(data))\n","\n","        else:\n","            a = self.importance(data, attributes)\n","            tree = Node(attribute=a, attribute_values=np.unique(data[:, a]), child_nodes=[])\n","            attributes.remove(a)\n","            for vk in np.unique(data[:, a]):\n","                new_data = data[data[:, a] == vk, :]\n","                subtree = self.train(new_data, attributes, data)\n","                tree.child_nodes.append(subtree)\n","\n","            return tree\n","\n","    def fit(self, data):\n","        self.root = self.train(data, list(range(data.shape[1] - 1)), np.array([]))\n","\n","    def predict(self, data):\n","        predictions = []\n","        for i in range(data.shape[0]):\n","            current_node = self.root\n","            while True:\n","                if current_node.decision is None:\n","                    current_attribute = current_node.attribute\n","                    current_attribute_value = data[i, current_attribute]\n","                    if current_attribute_value not in current_node.attribute_values:\n","                        predictions.append(random.randint(0, 1))\n","                        break\n","                    idx = list(current_node.attribute_values).index(current_attribute_value)\n","\n","                    current_node = current_node.child_nodes[idx]\n","                else:\n","                    predictions.append(current_node.decision)\n","                    break\n","\n","        return predictions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TTF_nt6GI2li"},"source":["dt = DecisionTree()\n","dt.fit(np.array(train_data))\n","predict = dt.predict(np.array(test_data))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hgh_XnB8L9q1"},"source":["test_data = np.array(test_data)\n","test_y = list(test_data[:, test_data.shape[1] - 1])\n","correct = 0\n","i = 0\n","for a in predict:\n","    if a == test_y[i]:\n","        correct += 1\n","    i += 1\n","accuracy = (correct/len(test_y))*100\n","tp = (1, 1)  # True positive\n","tn = (0, 0)  # True negative\n","fp = (1, 0)  # False positive\n","fn = (0, 1)  # False negative\n","mer = list(zip(predict, test_y))\n","true_positive = mer.count(tp)\n","true_negative = mer.count(tn)\n","false_positive = mer.count(fp)\n","false_negative = mer.count(fn)\n","precission = true_positive/(true_positive + false_positive)\n","recall = true_positive/(true_positive + false_negative)\n","f1_score = (2*precission*recall)/(precission + recall)\n","print(\"The accuracy of this algorithm is: {}%\".format(accuracy))\n","print(\"The precision value of this algorithm is: {}\".format(precission))\n","print(\"The recall value of this algorithm is: {}\".format(recall))\n","print(\"F1 score of this algorithm is: {}\".format(f1_score))"],"execution_count":null,"outputs":[]}]}